# MyAI - High-Performance Modular AI Assistant Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)

## Overview

MyAI is a production-ready, high-performance AI assistant framework designed for single developers to run or train on accessible hardware. It features:

- ğŸš€ **Fast Inference**: Optimized with quantization, batching, and async processing
- ğŸ”§ **Modular Architecture**: Easy to extend and customize
- ğŸ“Š **Full Observability**: Prometheus metrics, Grafana dashboards, structured logging
- ğŸ”’ **Security First**: OAuth2, rate limiting, content moderation
- ğŸ³ **Cloud Native**: Docker, Kubernetes, Helm charts included
- ğŸ¯ **Production Ready**: CI/CD, tests, monitoring, and deployment configs

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Model     â”‚
â”‚  (React)    â”‚â—€â”€â”€â”€â”€â”€â”‚   Backend    â”‚â—€â”€â”€â”€â”€â”€â”‚   Worker    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      â”‚
                            â–¼                      â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    Redis     â”‚      â”‚  GPU/CPU    â”‚
                     â”‚   (Cache)    â”‚      â”‚  Inference  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Prometheus  â”‚
                     â”‚   Grafana    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.9+
- Docker & Docker Compose
- Node.js 18+ (for frontend)
- CUDA 11.8+ (optional, for GPU acceleration)

### Local Development (CPU Mode)

```bash
# Clone the repository
git clone https://github.com/arup73735-lab/infinitiyai.git
cd infinitiyai

# Set up Python environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r backend/requirements.txt

# Start backend
cd backend
python main.py

# In another terminal, start frontend
cd frontend
npm install
npm run build  # Build for production

# Or use Docker Compose (recommended)
docker-compose up
```

### Access the Application

- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs
- Prometheus: http://localhost:9090
- Grafana: http://localhost:3001

## Project Structure

```
myai/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ RUNBOOK.md                     # Deployment guide
â”œâ”€â”€ DESIGN_EXPLAINER.md           # Architecture deep-dive
â”œâ”€â”€ DATA_PRIVACY.md               # Privacy policy
â”œâ”€â”€ SAFETY.md                     # Content moderation guide
â”œâ”€â”€ docker-compose.yml            # Multi-service orchestration
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”‚
â”œâ”€â”€ backend/                      # FastAPI inference server
â”‚   â”œâ”€â”€ main.py                   # API entry point
â”‚   â”œâ”€â”€ model_loader.py           # Model management
â”‚   â”œâ”€â”€ worker.py                 # Async batch worker
â”‚   â”œâ”€â”€ config.py                 # Configuration
â”‚   â”œâ”€â”€ auth.py                   # OAuth2 + JWT
â”‚   â”œâ”€â”€ middleware.py             # Rate limiting, logging
â”‚   â”œâ”€â”€ safety.py                 # Content moderation
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                # Backend container
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_api.py           # API tests
â”‚       â”œâ”€â”€ test_model.py         # Model tests
â”‚       â””â”€â”€ conftest.py           # Pytest fixtures
â”‚
â”œâ”€â”€ model/                        # Training & conversion
â”‚   â”œâ”€â”€ train.py                  # Fine-tuning pipeline
â”‚   â”œâ”€â”€ dataset.py                # Data processing
â”‚   â”œâ”€â”€ convert_to_onnx.py        # ONNX export
â”‚   â”œâ”€â”€ config.yaml               # Training config
â”‚   â””â”€â”€ requirements.txt          # Training dependencies
â”‚
â”œâ”€â”€ frontend/                     # React TypeScript UI
â”‚   â”œâ”€â”€ package.json              # NPM dependencies
â”‚   â”œâ”€â”€ tsconfig.json             # TypeScript config
â”‚   â”œâ”€â”€ vite.config.ts            # Vite bundler config
â”‚   â”œâ”€â”€ Dockerfile                # Frontend container
â”‚   â”œâ”€â”€ public/                   # Static assets
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx               # Main component
â”‚       â”œâ”€â”€ components/           # UI components
â”‚       â”œâ”€â”€ services/             # API client
â”‚       â””â”€â”€ types/                # TypeScript types
â”‚
â”œâ”€â”€ k8s/                          # Kubernetes manifests
â”‚   â”œâ”€â”€ deployment.yaml           # Backend deployment
â”‚   â”œâ”€â”€ service.yaml              # Service definitions
â”‚   â”œâ”€â”€ ingress.yaml              # Ingress rules
â”‚   â”œâ”€â”€ hpa.yaml                  # Horizontal Pod Autoscaler
â”‚   â””â”€â”€ configmap.yaml            # Configuration
â”‚
â”œâ”€â”€ helm/                         # Helm chart
â”‚   â””â”€â”€ myai/
â”‚       â”œâ”€â”€ Chart.yaml            # Chart metadata
â”‚       â”œâ”€â”€ values.yaml           # Default values
â”‚       â””â”€â”€ templates/            # K8s templates
â”‚
â”œâ”€â”€ monitoring/                   # Observability
â”‚   â”œâ”€â”€ prometheus.yml            # Prometheus config
â”‚   â””â”€â”€ grafana/
â”‚       â””â”€â”€ dashboards.json       # Grafana dashboard
â”‚
â”œâ”€â”€ perf/                         # Performance testing
â”‚   â”œâ”€â”€ benchmark.sh              # Benchmark script
â”‚   â””â”€â”€ load_test.py              # Load testing
â”‚
â””â”€â”€ ci/                           # CI/CD
    â””â”€â”€ github-actions.yml        # GitHub Actions workflow
```

## Features

### Inference Server
- Streaming token generation via WebSocket
- Automatic batching for throughput optimization
- Model quantization (INT8, FP16)
- Multi-GPU support with model parallelism
- Redis caching for repeated queries

### Training Pipeline
- Fine-tuning on custom datasets
- LoRA/QLoRA for efficient training
- Gradient accumulation for large batches
- Mixed precision training (AMP)
- Checkpoint management and resumption

### Security
- JWT-based authentication
- Rate limiting per user/IP
- Input sanitization and validation
- Content moderation filters
- Secrets management via environment variables

### Monitoring
- Prometheus metrics (latency, throughput, errors)
- Grafana dashboards
- Structured JSON logging
- Sentry error tracking
- Health check endpoints

## Documentation

- [RUNBOOK.md](RUNBOOK.md) - Deployment and operations guide
- [DESIGN_EXPLAINER.md](DESIGN_EXPLAINER.md) - Architecture and design decisions
- [DATA_PRIVACY.md](DATA_PRIVACY.md) - Privacy and data handling
- [SAFETY.md](SAFETY.md) - Content moderation and safety

## Performance

### Benchmarks (on NVIDIA A100 40GB)

| Model Size | Batch Size | Throughput | Latency (p50) | Latency (p95) |
|------------|------------|------------|---------------|---------------|
| 125M       | 1          | 45 tok/s   | 22ms          | 35ms          |
| 125M       | 8          | 280 tok/s  | 28ms          | 45ms          |
| 1.3B       | 1          | 28 tok/s   | 35ms          | 52ms          |
| 1.3B       | 8          | 180 tok/s  | 44ms          | 68ms          |

### Cost Estimates

**Training (Fine-tuning 1.3B model on 10GB data)**
- 8x A100 (80GB): ~$25/hour Ã— 12 hours = $300
- Single A100 (40GB) with gradient accumulation: ~$3/hour Ã— 48 hours = $144

**Inference (1M tokens/day)**
- Cloud GPU (A10G): ~$0.50/hour Ã— 24 = $12/day
- Optimized CPU (32 cores): ~$0.15/hour Ã— 24 = $3.60/day

## Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

For issues and questions:
- GitHub Issues: https://github.com/arup73735-lab/infinitiyai/issues
- Documentation: https://github.com/arup73735-lab/infinitiyai/wiki

## Acknowledgments

- Hugging Face Transformers
- FastAPI
- React and TypeScript community
- Open source AI community
